{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import itertools\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# read entity function\n",
    "def read_ent(toks):\n",
    "    prev_ent = None\n",
    "    start_ent = 0   \n",
    "    ent_list = {}\n",
    "    for i,x in enumerate(toks):\n",
    "        ll = \"-\".join(x.split(\"-\")[1:])\n",
    "        if ll!=prev_ent:\n",
    "            if prev_ent!=None and prev_ent!=\"\":\n",
    "\n",
    "                ent_list[prev_ent] =(start_ent,i)\n",
    "            start_ent=i\n",
    "            prev_ent=ll\n",
    "    if ll!=\"0\":\n",
    "        ent_list[prev_ent] =(start_ent,i+1)\n",
    "    return ent_list\n",
    "\n",
    "\n",
    "parsed_file = \"gutenberg_parse_ie_etexts.json\"\n",
    "outfile = \"gutenberg_network_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reading file: {}\".format(\"gutenberg_parse_ie_etexts.json\"))\n",
    "sent_graph = []\n",
    "sent_vol = set()\n",
    "with open(parsed_file,\"r\") as file:\n",
    "    for l in file:\n",
    "        xx = json.loads(l)\n",
    "\n",
    "        sent_graph.append(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare sentence graph for each volume\n"
     ]
    }
   ],
   "source": [
    "# prepare sentence graph for each volume\n",
    "print(\"prepare sentence graph for each volume\")\n",
    "pd_graph = pd.DataFrame(sent_graph)\n",
    "pd_graph[\"vol_id\"] = pd_graph.sent_id.apply(lambda xx:\"-\".join(xx.split(\"-\")[:-1]))\n",
    "\n",
    "# for sample take numbers\n",
    "n_samples = 20\n",
    "vol_list = list(pd_graph.vol_id.unique())[:n_samples]\n",
    "\n",
    "pd_graph = pd_graph[pd_graph.vol_id.apply(lambda x:x in vol_list)]\n",
    "\n",
    "sent_graph_pd = pd_graph[pd_graph.openie.apply(lambda x:len(x[\"verbs\"])>0)]\n",
    "\n",
    "vol_ids = sent_graph_pd.vol_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define global volume graph for 40 volumes\n"
     ]
    }
   ],
   "source": [
    "# define global volume graph\n",
    "print(\"define global volume graph for {} volumes\".format(n_samples))\n",
    "vol_graph = {}\n",
    "\n",
    "for vol in tqdm.tqdm(vol_ids):\n",
    "    test_pd = sent_graph_pd.groupby(\"vol_id\").get_group(vol)\n",
    "    import numpy as np\n",
    "    graph_list = []\n",
    "    for x in test_pd[[\"sent_id\",\"openie\",\"entities\",\"token\"]].to_records():\n",
    "        temp_graph = nx.DiGraph()\n",
    "        for y in x.openie[\"verbs\"]:\n",
    "            ent_ext = x.entities\n",
    "            tokens = x.openie[\"words\"]\n",
    "            net_list = []\n",
    "            for xx in ent_ext:    \n",
    "                mask_ent = np.zeros(len(y[\"tags\"]),dtype=int)\n",
    "                mask_ent[xx[2]:xx[3]] = 1\n",
    "                S = None\n",
    "                V = None\n",
    "                O = None\n",
    "                all_ent = read_ent(y[\"tags\"])\n",
    "                try:\n",
    "                    V_pos = all_ent[\"V\"]  \n",
    "                except:\n",
    "                    continue\n",
    "                V = tokens[V_pos[0]:V_pos[1]]\n",
    "                for key,yy in all_ent.items():                \n",
    "                    if key == \"V\":\n",
    "                        continue\n",
    "\n",
    "                    mask = np.zeros(len(y[\"tags\"]),dtype=int)\n",
    "                    mask[yy[0]:yy[1]] = 1\n",
    "                    intersect = np.sum(mask&mask_ent)\n",
    "                    if intersect>0:\n",
    "                        if yy[0] < V_pos[0]:\n",
    "                            S = tokens[xx[2]:xx[3]]\n",
    "                        else:\n",
    "                            O = tokens[xx[2]:xx[3]]\n",
    "\n",
    "                        temp_graph.add_node(\" \".join(V),stype=\"v\")                    \n",
    "                        if (xx[0] == \"PERSON\") | (xx[0] == \"ORG\"):\n",
    "                            if S is not None:\n",
    "                                temp_graph.add_edge(\" \".join(S),\" \".join(V),ie=(xx,y))\n",
    "                            if O is not None:                        \n",
    "                                temp_graph.add_edge(\" \".join(V),\" \".join(O),ie=(xx,y))\n",
    "\n",
    "\n",
    "        required_max_path_length = 3 # (inferior or equal to)\n",
    "\n",
    "        G = temp_graph\n",
    "\n",
    "        all_paths = []\n",
    "        nodes_combs = itertools.permutations(G.nodes, 2)\n",
    "\n",
    "        for source, target in nodes_combs:\n",
    "            paths = nx.all_simple_paths(G, source=source, target=target, cutoff=required_max_path_length)\n",
    "\n",
    "            for path in paths:\n",
    "                if path not in all_paths and path[::-1] not in all_paths and len(path)==required_max_path_length:\n",
    "                    if path[0] not in nx.get_node_attributes(G,\"stype\").keys():\n",
    "                        all_paths.append(path)\n",
    "\n",
    "\n",
    "        if len(all_paths)>0:                    \n",
    "            graph_list.append((x.sent_id,temp_graph,all_paths,x.token))\n",
    "\n",
    "    vol_graph[vol]=graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:05,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing network and sentiment analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:04<00:00,  6.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute sentiment anlaysis \n",
    "print(\"preparing network and sentiment analysis\")\n",
    "cons_graph_vol = {}\n",
    "\n",
    "for xvol,graph_list in tqdm.tqdm(vol_graph.items()):\n",
    "    cons_graph = nx.DiGraph()\n",
    "    for x in graph_list:\n",
    "        is_break = False\n",
    "        for iy,y in enumerate(x[2]):        \n",
    "            if cons_graph.get_edge_data(y[0], y[2]) == None:\n",
    "                cons_graph.add_edge(y[0],y[2],etype=[y[1]],ie=[(x[0],x[3],y[1],nx.get_edge_attributes(x[1],\"ie\")[(y[0],y[1])][1]\n",
    "    ,nx.get_edge_attributes(x[1],\"ie\")[(y[1],y[2])][1])])\n",
    "            else:\n",
    "                cons_graph.get_edge_data(y[0], y[2])[\"ie\"].append((x[0],x[3],y[1],nx.get_edge_attributes(x[1],\"ie\")[(y[0],y[1])][1]\n",
    "    ,nx.get_edge_attributes(x[1],\"ie\")[(y[1],y[2])][1]))\n",
    "                cons_graph.get_edge_data(y[0], y[2])[\"etype\"].append(y[1])\n",
    "            is_break = True\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    rel_list = []\n",
    "    for key,value in nx.get_edge_attributes(cons_graph,\"ie\").items():\n",
    "        for y in value:\n",
    "            temp_tags = y[4][\"tags\"]\n",
    "            temp_tags = temp_tags + (len(y[1]) - len(y[4][\"tags\"])) * [\"O\"]\n",
    "            try:\n",
    "                sentence = \" \".join(np.array(y[1])[np.array(temp_tags)!=\"O\"])\n",
    "            except BaseException as ex:\n",
    "                raise ex\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            sent_id = y[0]\n",
    "            time_frame = int(sent_id.split(\"-\")[-1])\n",
    "            rel_list.append((sent_id,time_frame,key[0],key[1],sentence,vs,y,y[4][\"verb\"]))\n",
    "            rel_list.append((sent_id,time_frame,key[1],key[0],sentence,vs,y,y[4][\"verb\"]))\n",
    "\n",
    "    cons_graph_vol[xvol] = {\"cons_graph\": cons_graph,\"rel_list\": pd.DataFrame(rel_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataframe for network relations and sentiment dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [06:55<00:00, 13.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# prepare dataframe for sentiment analysis and network relations\n",
    "print(\"Preparing dataframe for network relations and sentiment dynamic\")\n",
    "new_frame = pd.DataFrame()\n",
    "\n",
    "for xvol,val in tqdm.tqdm(cons_graph_vol.items()):\n",
    "    rel_frame = val[\"rel_list\"]\n",
    "    rel_frame[\"sentiment\"] = rel_frame[5].apply(lambda x: x[\"pos\"] - x[\"neg\"])\n",
    "    rel_frame_group = rel_frame.groupby([1,2,3]).mean().reset_index()\n",
    "        \n",
    "    for x in rel_frame_group[[2,3]].groupby([2,3]).count().index:\n",
    "        rel_frame_gg = rel_frame.groupby([2,3]).get_group(tuple(x))\n",
    "        rel_frame_gg = rel_frame_gg.sort_values(1)\n",
    "        rel_frame_gg[\"sentiment_rolling\"] = rel_frame_gg[[\"sentiment\"]].rolling(window=3,min_periods=1).mean()\n",
    "        rel_frame_gg[\"delta_sentiment\"] = abs(rel_frame_gg[[\"sentiment\"]] - rel_frame_gg[[\"sentiment\"]].shift(1))\n",
    "        rel_frame_gg[\"delta_sum\"] = rel_frame_gg.fillna(0).delta_sentiment.rolling(window=2).sum()\n",
    "        rel_frame_gg[\"delta_time\"] = rel_frame_gg[[1]] - rel_frame_gg[[1]].shift(1)\n",
    "        rel_frame_gg[\"vol_id\"] = xvol\n",
    "        new_frame = new_frame.append(rel_frame_gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_columns = list(new_frame.columns)\n",
    "temp_columns[0] = \"volume_id\"\n",
    "temp_columns[1] = \"sentence_id\"\n",
    "temp_columns[2] = \"actor_a\"\n",
    "temp_columns[3] = \"actor_b\"\n",
    "temp_columns[4] = \"sentence_part\"\n",
    "temp_columns[5] = \"sentiment\"\n",
    "temp_columns[6] = \"tokens\"\n",
    "temp_columns[7] = \"verbs\"\n",
    "new_frame.columns = temp_columns\n",
    "new_frame = new_frame.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished, saved output file to gutenberg_network_data.csv\n"
     ]
    }
   ],
   "source": [
    "# save network and sentiment frame\n",
    "new_frame.to_csv(outfile)\n",
    "print(\"Finished, saved output file to {}\".format(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6064b8564c6bb04ebb296b38dac807d69b2d3afbbe3906cd8162400e9639f86"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('python_37': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
